{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and split it into trainings, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_pickle('dataset.pkl')\n",
    "\n",
    "# Extract the validation data from the dataset\n",
    "val_data = data.loc[data.run >= 97].drop('run', axis=1)\n",
    "data = data.loc[data.run <= 96].drop('run', axis=1)\n",
    "\n",
    "# Split the remaining dataset into trainings and test data\n",
    "train_complete = data[:int(len(data)*0.75)]\n",
    "test_complete = data[int(len(data)*0.75):]\n",
    "\n",
    "# Method to drop the less important features (-> Random Forest)\n",
    "def drop_less_important(data_temp, drop_many=True):\n",
    "    if drop_many:\n",
    "        data_temp = data_temp.drop([''], axis=1, errors='ignore')\n",
    "    return data_temp.drop(['clusterAbsZernikeMoment40', 'clusterAbsZernikeMoment51', 'clusterLAT', 'phi', 'b2bPhi', 'cosAngleBetweenMomentumAndVertexVectorInXYPlane','pRecoilPhi'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the dataframe containing the pair of photons into two single dataframe containing each on photon and\n",
    "# one dataframe containing the label\n",
    "def split_sets(data_temp):\n",
    "    simple_column_names = ['clusterAbsZernikeMoment40', 'clusterAbsZernikeMoment51', 'clusterErrorTiming', 'clusterLAT', 'clusterTiming', 'clusterZernikeMVA', 'phi', 'pz', \n",
    "             'b2bPhi', 'cosAngleBetweenMomentumAndVertexVector', 'cosAngleBetweenMomentumAndVertexVectorInXYPlane', 'pRecoilPhi', 'pxErr', 'pyErr', 'pzErr', 'minC2TDist']\n",
    "    data_temp_1 = pd.DataFrame(data_temp[[feature + '_1' for feature in simple_column_names]].values, columns=simple_column_names)\n",
    "    data_temp_2 = pd.DataFrame(data_temp[[feature + '_2' for feature in simple_column_names]].values, columns=simple_column_names)\n",
    "\n",
    "    return data_temp_1, data_temp_2, data_temp.y\n",
    "\n",
    "# Method to get data. The trainigs dataset can be reduced by a factor and the less important features can be dropped.\n",
    "def get_data(factor=1, important_drop=False):\n",
    "\n",
    "    train_1, train_2, train_y = split_sets(train_complete[:(len(train_complete)//factor)])\n",
    "    test_1, test_2, test_y = split_sets(test_complete)\n",
    "    val_1, val_2, val_y = split_sets(val_data)\n",
    "    \n",
    "    if important_drop:\n",
    "        train_1, train_2 = drop_less_important(train_1), drop_less_important(train_2)\n",
    "        test_1, test_2 = drop_less_important(test_1), drop_less_important(test_2)\n",
    "        val_1, val_2 = drop_less_important(val_1), drop_less_important(val_2)\n",
    "    \n",
    "    print('Length trainings data:', len(train_1), 'Length test data:', len(test_1), 'Length validation data:', len(val_1), '\\n')\n",
    "    return [train_1, train_2, train_y], [test_1, test_2, test_y], [val_1, val_2, val_y]\n",
    "\n",
    "def get_columns():\n",
    "    return val_data.columns\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the trainings, test and validation data\n",
    "factor = 1\n",
    "train, test, val = get_data(factor=factor, important_drop=True)\n",
    "print(train[0].columns)\n",
    "\n",
    "# Define a dropout rate for the 'Dropout' layer\n",
    "rate = 0.25\n",
    "\n",
    "# Create the input layers for the two identical neural networks\n",
    "input_shape_single = len(train[0].columns)\n",
    "input_shape_l = keras.layers.Input((input_shape_single,))\n",
    "input_shape_r = keras.layers.Input((input_shape_single,))\n",
    "output_nodes_last_single_layer = 96\n",
    "\n",
    "\n",
    "# Create the neural network the first two neural networks share\n",
    "model_simple = keras.Sequential([\n",
    "    keras.layers.Input(shape=(input_shape_single,)),\n",
    "    keras.layers.Dense(128, activation='tanh'),\n",
    "    #keras.layers.Dropout(rate, noise_shape=None),\n",
    "    keras.layers.Dense(64, activation='tanh'),\n",
    "    #keras.layers.Dropout(rate, noise_shape=None),\n",
    "    keras.layers.Dense(output_nodes_last_single_layer, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Call the the created network on each of the input tensors so the weights will be shared\n",
    "encoded_l = model_simple(input_shape_l)\n",
    "encoded_r = model_simple(input_shape_r)\n",
    "    \n",
    "# Create a layer to merge the output of the two neural networks together\n",
    "L1_layer = tf.keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "\n",
    "# Call this layer with the two previously created networks\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "# Create the neural network for the merged values\n",
    "x = keras.layers.Dense(output_nodes_last_single_layer, activation='tanh')(L1_distance)\n",
    "#x = keras.layers.Dropout(rate, noise_shape=None)(x)\n",
    "x = keras.layers.Dense(32, activation='tanh')(x)\n",
    "#x = keras.layers.Dropout(rate, noise_shape=None)(x)\n",
    "x = keras.layers.Dense(8, activation='tanh')(x)\n",
    "\n",
    "# Create a output layer and combine all the models into one\n",
    "prediction = keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "siamese_net = keras.models.Model(inputs=[input_shape_l,input_shape_r],outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "# Set the parameters for the training including early stopping when the validation loss increases (-> overfitting)\n",
    "num_of_epochs = 350\n",
    "batch_size = 1024\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300)\n",
    "\n",
    "# Train the network\n",
    "train_history = siamese_net.fit(train[0:2], train[2], verbose=1, batch_size=batch_size, epochs=num_of_epochs, validation_data=[val[0:2], val[2]], callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy and loss during training and the best validation accuracy during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(loss, acc):\n",
    "    # Plot the trainings and accuracy history\n",
    "    \n",
    "    _, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12,8))\n",
    "    \n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.plot(loss, '-o')\n",
    "    \n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_xlabel('Epoch Index')\n",
    "    ax2.plot(acc, '-s')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training(train_history.history['loss'], train_history.history['acc'])\n",
    "plot_training(train_history.history['val_loss'], train_history.history['val_acc'])\n",
    "\n",
    "\n",
    "print('\\nHighest accuracy on the validation dataset during the trainings process:')\n",
    "print(max(train_history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model: Prints the accuracy and F1-score for the trainings and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and F1-score for the test and validation data\n",
    "y_pred = np.vectorize(round)(siamese_net.predict(test[0:2]))\n",
    "print('\\nAccuracy on test data:', accuracy_score(test[2], y_pred))\n",
    "print('F1-score on test data:', f1_score(test[2], y_pred))\n",
    "print('Precision on test data:', precision_score(test[2], y_pred))\n",
    "\n",
    "y_pred = np.vectorize(round)(siamese_net.predict(val[0:2]))\n",
    "print('\\nAccurcay on validation data:', accuracy_score(val[2], y_pred))\n",
    "print('F1-score on validation data:', f1_score(val[2], y_pred))\n",
    "print('Precision on validation data:', precision_score(val[2], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Precision-Recall curve and compute the AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the predictions of the classifier as probabilities\n",
    "y_pred = siamese_net.predict(val[0:2])\n",
    "\n",
    "# Compute precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(val[2], y_pred)\n",
    "\n",
    "# Computes the area under curve from the precision and recall\n",
    "auc_score = auc(recall, precision)\n",
    "print('Area under curve:', auc_score)\n",
    "\n",
    "# Plot the precision-recall curves\n",
    "plt.plot([0,1], [.33, .33], linestyle='-', label='Random')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot(recall, precision, linestyle='-', label='Neural Network')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save predictions and trainings history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_9999_data.npy'\n",
    "data = [siamese_net.predict(test[0:2]), siamese_net.predict(val[0:2]), train_history.history]\n",
    "np.save(filename, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
