{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to load, create, and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import root_pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to process a list of given DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filename = '../RECO/RECO_out_'\n",
    "custom_ending = '_pi0_df.root'\n",
    "# path = custom_filename + number of run + custom_ending, e.g. '../RECO/RECO_out_1_pi0_df.root'\n",
    "num_of_runs = 4\n",
    "\n",
    "def shuffle_df(data):\n",
    "    # Shuffes a given DataFrame\n",
    "\n",
    "    indices = np.arange(data.values.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return pd.DataFrame(data.values[indices], columns=data.columns)\n",
    "\n",
    "def process_data(data_temp, i=-1):\n",
    "    data_temp = cut_pi0s(data_temp)\n",
    "    data_temp = create_dataset(data_temp)\n",
    "    data_temp = normalize_data(data_temp)\n",
    "    data_temp['run'] = i\n",
    "    \n",
    "    return data_temp\n",
    "\n",
    "print('-'*30, '\\nDATASET 1\\n')\n",
    "#data = process_data(root_pandas.read_root('/media/sf_shared_folder/RECO/RECO_out_1_pi0.root'), 1)\n",
    "data = process_data(pd.read_pickle(custom_filename + '1' + custom_ending), 1)\n",
    "\n",
    "for i in np.arange(2,(num_of_runs+1)):\n",
    "    print('\\n', '-'*30, '\\nDATASET ', i, '\\n', sep='')    \n",
    "    \n",
    "    path = custom_filename + str(i) + custom_ending\n",
    "    #pi0s = root_pandas.read_root(path)\n",
    "    pi0s = pd.read_pickle(path)\n",
    "    \n",
    "    data = pd.concat([data, process_data(pi0s, i)])\n",
    "\n",
    "data = shuffle_df(data)\n",
    "data.to_pickle('dataset.pkl')\n",
    "\n",
    "print('\\n', '-'*30, '\\nDone\\n', sep='')\n",
    "print(sum(data.y), len(data.y))\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wanted_gammas(gammas_temp, p_cut=0.13):\n",
    "    # Get a DataFrame with all wanted gammas\n",
    "    return copy.deepcopy(gammas_temp.loc[(abs(gammas_temp.genMotherPDG__bo1__bc)==423) & (abs(gammas_temp.genMotherPDG)==111) & (gammas_temp.isSignal==1) & (gammas_temp.p < p_cut)])\n",
    "\n",
    "def get_wanted_pi0s(pi0s_temp, p_cut=0.23):\n",
    "    # Get a DataFrame with all wanted pi0s\n",
    "    return copy.deepcopy(pi0s_temp.loc[(abs(pi0s_temp.genMotherPDG)==423) & (pi0s_temp.isSignal==1) & (pi0s_temp.p < p_cut)])\n",
    "\n",
    "def compute_cut_efficiency(before_cut_data, after_cut_data, is_pi0s=False):\n",
    "    # Compute the efficiency of a cut\n",
    "    \n",
    "    if is_pi0s:\n",
    "        result = np.array([(len(after_cut_data)/len(before_cut_data)), (len(get_wanted_pi0s(after_cut_data))/len(get_wanted_pi0s(before_cut_data))), len(get_wanted_pi0s(after_cut_data)), len(after_cut_data)])\n",
    "    else:\n",
    "        result = np.array([(len(after_cut_data)/len(before_cut_data)), (len(get_wanted_gammas(after_cut_data))/len(get_wanted_gammas(before_cut_data))), len(get_wanted_gammas(after_cut_data)), len(after_cut_data)])\n",
    "\n",
    "    # The array contains the following information:\n",
    "    # [% of remaining dataset, % of remaining wanted data, # of remaining wanted datapoints, # of remaining datapoints]\n",
    "    print(result[0], result[1], '\\n', result[2], result[3], '\\n') # TODO format output\n",
    "    return result\n",
    "\n",
    "#NOTE: Note: PDG: 111: pi0 ; 423: D*(2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut the $\\pi^0$ list. Keep only $\\pi^0$'s where dM is in the range -0.075 and 0.02 and where the momentum is less than 0.23 GeV/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_pi0s(pi0s_temp):\n",
    "    pi0s_cutted_temp = copy.deepcopy(pi0s_temp.loc[abs(pi0s_temp.dM > -75E-3) & abs(pi0s_temp.dM < 2E-2) & (pi0s_temp.p > 0.23)])\n",
    "    compute_cut_efficiency(pi0s_temp, pi0s_cutted_temp, is_pi0s=True)\n",
    "    return pi0s_cutted_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing all $\\gamma$ combinations based on the $\\pi^0$ list. The column names of the first $\\gamma$ have '_2' appended and the first gamma '_1'.\n",
    "\n",
    "Note: All combinations are only created in one permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(genMotherPDG, isSignal):\n",
    "    return int((abs(genMotherPDG) == 423) & (isSignal == 1))\n",
    "\n",
    "def simple_drop(y, drop_probability):\n",
    "    return ((y==1) | (random.uniform(0, 1) > drop_probability))\n",
    "\n",
    "def create_dataset(pi0s, drop_probability=0.95):\n",
    "\n",
    "    data_temp = copy.deepcopy(pi0s)\n",
    "\n",
    "    #data_temp['y'] = data_temp.apply(lambda x: int((abs(x.genMotherPDG) == 423) & (x.isSignal == 1)), axis=1)\n",
    "    vfunc = np.vectorize(get_y)\n",
    "    data_temp['y'] = vfunc(data_temp.genMotherPDG, data_temp.isSignal)\n",
    "    \n",
    "    #data_temp['flag'] = data_temp.apply(lambda x: ( (x.y==1) | (random.uniform(0, 1) > drop_probability)), axis=1)\n",
    "    vfunc = np.vectorize(simple_drop)\n",
    "    data_temp['flag'] = vfunc(data_temp.y, drop_probability)\n",
    "    \n",
    "    data_temp = data_temp.loc[data_temp.flag==1].drop('flag', axis=1)\n",
    "\n",
    "    data_temp.drop(['__experiment__', '__run__', '__event__', '__candidate__', '__ncandidates__', '__weight__', 'genMotherID', \n",
    "                 'genMotherPDG', 'genMotherPDG__bo1__bc', 'genMotherPDG__bo2__bc', 'isSignal', 'mdstSource',\n",
    "                   'daughter__bo0__cm__spmdstSource__bc', 'daughter__bo1__cm__spmdstSource__bc', 'p', 'dM']\n",
    "                   , axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Print some info about the created dataset\n",
    "    print('Size of the dataset: (x_data, y_data)', len(data_temp))\n",
    "    print('Number of real pi0 combinations:', sum(data_temp['y']))\n",
    "    \n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data that all values have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = load(open('scaler.pkl', 'rb'))\n",
    "\n",
    "def normalize_data(data_temp, scale_localy=False):\n",
    "    y_values = data_temp.y\n",
    "    data_temp = data_temp.drop(['y'], axis=1)\n",
    "    \n",
    "    if scale_localy:\n",
    "        data_temp[data_temp.columns] = scaler.fit_transform(data_temp[data_temp.columns])\n",
    "    else:\n",
    "        data_temp[data_temp.columns] = scaler.transform(data_temp[data_temp.columns])\n",
    "        \n",
    "    data_temp['y'] = y_values\n",
    "    \n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list where the amount of wanted and unwanted combinations is balanced and save it in files. The dataset is balanced by simply dropping some unwanted combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_data(data_temp, drop_propability):\n",
    "    # Function that drops an unwanted combination with a given probability\n",
    "    \n",
    "    data_temp = copy.deepcopy(data_temp)\n",
    "\n",
    "    data_temp['flag'] = data_temp.apply(lambda x: ( (x.y==1) | (random.uniform(0, 1) > drop_propability)), axis=1)\n",
    "    result = data_temp.loc[data_temp.flag==1].drop('flag', axis=1)\n",
    "    \n",
    "    print(sum(result.y), len(result.y))    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a StandardScaler with the 10 first dataset and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_data(data_temp):\n",
    "    data_temp = cut_pi0s(data_temp)\n",
    "    data_temp.drop(['__experiment__', '__run__', '__event__', '__candidate__', '__ncandidates__', '__weight__', 'genMotherID', \n",
    "                 'genMotherPDG', 'genMotherPDG__bo1__bc', 'genMotherPDG__bo2__bc', 'isSignal', 'mdstSource',\n",
    "                   'daughter__bo0__cm__spmdstSource__bc', 'daughter__bo1__cm__spmdstSource__bc', 'p', 'dM']\n",
    "                , axis=1, inplace=True, errors='ignore')\n",
    "    print(len(data_temp.columns))\n",
    "    \n",
    "    return data_temp\n",
    "\n",
    "print('-'*30, '\\nDATASET 1\\n')\n",
    "data = shorten_data(pd.read_pickle('../RECO/RECO_out_1_pi0_df.root'))\n",
    "\n",
    "for i in np.arange(2,51):\n",
    "    print('\\n', '-'*30, '\\nDATASET ', i, '\\n', sep='')    \n",
    "    \n",
    "    path = '../RECO/RECO_out_' + str(i) + '_pi0_df.root'\n",
    "    data = pd.concat([data, shorten_data(pd.read_pickle(path))])\n",
    "    print(data.shape)\n",
    "    \n",
    "    \n",
    "scaler = StandardScaler().fit(data[data.columns])\n",
    "\n",
    "dump(scaler, open('scaler.pkl', 'wb'))\n",
    "\n",
    "del data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove collinear columns. To remove collinear columns from the data I am using VIF (https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/) with a threshold of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh=10.0):\n",
    "    # https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python\n",
    "\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n",
    "                  '\\' at index: ' + str(maxloc))\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables]\n",
    "\n",
    "def remove_collinear_rows(gammas_temp):\n",
    "    # Removes collinear columns but ignores the mdstSource column\n",
    "    gammas_temp = copy.deepcopy(gammas_temp)\n",
    "    mdstSource_temp = gammas_temp.mdstSource\n",
    "    gammas_temp.drop(['__experiment__', '__run__', '__event__', '__candidate__', '__ncandidates__', '__weight__', 'genMotherID',\n",
    "                 'genMotherP','M', 'dM', 'InvM', 'E', 'E_uncertainty', 'clusterE9E21', 'b2bClusterTheta', 'b2bClusterPhi',\n",
    "                 'clusterPhi', 'clusterErrorPhi', 'clusterTheta', 'clusterE', 'clusterErrorE', 'ECLClusterE_uncertainty', \n",
    "                 'genMotherPDG', 'genMotherPDG__bo1__bc', 'genMotherPDG__bo2__bc', 'isSignal', 'mdstSource']\n",
    "                , axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    return pd.concat([mdstSource_temp, calculate_vif_(X=gammas_temp)], axis=1, join='inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
